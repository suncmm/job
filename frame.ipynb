{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Based On Selenium Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%file SeleniumLiepin.py\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Initialize variable \n",
    "page_number = 3\n",
    "info_df = pd.DataFrame()\n",
    "\n",
    "# Set up Edge driver\n",
    "opt = webdriver.EdgeOptions()   # Create setup object\n",
    "opt.add_argument('--disable-gpu')\n",
    "driver = webdriver.Edge(options=opt)\n",
    "\n",
    "# Wait for element locating\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Open the URL\n",
    "try:\n",
    "    # url = \"https://www.liepin.com\"\n",
    "    url = 'https://www.liepin.com/zhaopin/?city=410&dq=410&pubTime=&currentPage=3&pageSize=40&key=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%B7%A5%E7%A8%8B%E5%B8%88&suggestTag=&workYearCode=0&compId=&compName=&compTag=&industry=&salary=&jobKind=&compScale=&compKind=&compStage=&eduLevel=&otherCity=&sfrom=search_job_pc&ckId=01q2gz6bfx7lccu04cvnflv403m991qb&scene=page&skId=01q2gz6bfx7lccu04cvnflv403m991qb&fkId=01q2gz6bfx7lccu04cvnflv403m991qb&suggestId='\n",
    "    driver.get(url)\n",
    "\n",
    "    # # Type and search job position\n",
    "    # # Locate the `input` element and entry key word\n",
    "    # driver.find_element(By.XPATH, \"//input[@placeholder='搜索职位/公司/内容关键词']\").send_keys(\"自然语言处理工程师\")\n",
    "    # # Locate the search button element and click the button\n",
    "    # driver.find_element(By.XPATH, \"//span[text()='搜索']\").click()\n",
    "\n",
    "    # Switch to recruitment page\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "    while True:\n",
    "        tmp_data_list = []\n",
    "        tmp_data_df = pd.DataFrame()\n",
    "\n",
    "        # Wait for page loading to be completed\n",
    "        # while not driver.execute_script(\"document.readyState == 'complete'\"):\n",
    "        #     time.sleep(1)\n",
    "        WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, \"//div[@id='lp-search-job-box']/div[3]/section[1]/div[1]/div[40]\")))\n",
    "        eles = driver.find_elements(By.XPATH, \"//div[@id='lp-search-job-box']/div[3]/section[1]/div[1]/div\")\n",
    "\n",
    "        # Extracte job information\n",
    "        for ele in eles:\n",
    "            tmp_data_dict = dict()\n",
    "            try:\n",
    "                tmp_data_dict['position'] = ele.find_element(By.XPATH, \"./div/div[1]/div/a/div[1]/div/div[1]\").text\n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['position'] = 'N/A'  \n",
    "            try:\n",
    "                tmp_data_dict['city'] = ele.find_element(By.XPATH, \"./div/div[1]/div/a/div[1]/div/div[2]/span[2]\").text  \n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['city'] = 'N/A'  \n",
    "            try:\n",
    "                tmp_data_dict['payment'] = ele.find_element(By.XPATH, \"./div/div[1]/div/a/div[1]/span[@class='jsx-2693574896 job-salary']\").text\n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['payment'] = 'N/A'  \n",
    "            try:\n",
    "                tmp_data_dict['enterprise'] = ele.find_element(By.XPATH, \"./div/div[1]/div/div/div/span\").text\n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['enterprise'] = 'N/A'  \n",
    "            try:\n",
    "                tmp_data_dict['requirement-list'] = ele.find_element(By.XPATH, \"./div/div[1]/div/a/div[2]\").text\n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['requirement-list'] = 'N/A'  \n",
    "            try:\n",
    "                tmp_data_dict['CPprofile-list'] = ele.find_element(By.XPATH, \"./div/div[1]/div/div/div/div[2]\").text\n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['CPprofile-list'] = 'N/A'  \n",
    "\n",
    "            # # Wait for element to be visible and scroll to the visible area\n",
    "            # driver.execute_script('arguments[0].scrollIntoView();', ele)\n",
    "            # ele = WebDriverWait(driver, 10).until(EC.element_to_be_clickable(ele))\n",
    "            \n",
    "            # Open job information detail page\n",
    "            ele.click()\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            time.sleep(60)\n",
    "\n",
    "            # Extract job information detail\n",
    "            # Job description\n",
    "            try:\n",
    "                tmp_data_dict['job-detail'] = driver.find_element(By.XPATH, \"//dd[@data-selector='job-intro-content']\").text   # responsibilities and requirement-detail\n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['job-detail'] = 'N/A'  \n",
    "            # Profile-detail\n",
    "            try:\n",
    "                tmp_data_dict['enterprise-detail'] = driver.find_element(By.XPATH, \"//div[@class='paragraph-box']//div[1]\").text   # enterprise profile-detail\n",
    "            except NoSuchElementException:\n",
    "                tmp_data_dict['enterprise-detail'] = 'N/A'  \n",
    "            tmp_data_list.append(tmp_data_dict)\n",
    "            info_list.append(tmp_data_dict)\n",
    "\n",
    "            # Return overview page\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            # time.sleep(5)\n",
    "\n",
    "        # Save current page data\n",
    "        with open(f'tmp_data_dir/{page_number}.json', 'w') as ftd:\n",
    "            json.dump(tmp_data_list, ftd, ensure_ascii=False)\n",
    "        tdf = tmp_data_df.from_records(tmp_data_list)\n",
    "        tdf.to_csv(f'tmp_data_dir/{page_number}.csv', index=False)\n",
    "\n",
    "        # Go to next page\n",
    "        next_button = driver.find_element(By.XPATH, \"//li[@title='Next Page']//button[1]\")\n",
    "        if next_button.get_attribute('disabled') == 'true':\n",
    "            break\n",
    "        else:\n",
    "            next_button.click()\n",
    "            page_number += 1\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Capture exceptions and exit the browser\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Save information data\n",
    "with open('job1.json', 'w') as f:\n",
    "    json.dump(info_list, f, ensure_ascii=False)\n",
    "\n",
    "df = info_df.from_records(info_list)\n",
    "df.to_csv('job1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print(len(info_list))\n",
    "# print(type(info_list[2]['job-detail'].encode('unicode_escape').decode('utf-8')))\n",
    "# pprint(info_list[60:])\n",
    "\n",
    "# s = info_list[60:]\n",
    "# def find_incompatible_characters(string):\n",
    "#     # 定义匹配非法字符的正则表达式\n",
    "#     pattern = re.compile(r'[^\\x09\\x0A\\x0D\\x20-\\uD7FF\\uE000-\\uFFFD\\u10000-\\u10FFFF]')\n",
    "\n",
    "#     # 查找非法字符\n",
    "#     incompatible_characters = re.findall(pattern, string)\n",
    "\n",
    "#     return incompatible_characters\n",
    "\n",
    "# # 示例用法\n",
    "# string = \"Hello, <world>!\"\n",
    "# incompatible_chars = find_incompatible_characters(s)\n",
    "# if incompatible_chars:\n",
    "#     print(\"字符串中存在与XML不兼容的字符：\", incompatible_chars)\n",
    "# else:\n",
    "#     print(\"字符串中不存在与XML不兼容的字符\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Based On Requests Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%file liepin.py\n",
    "\n",
    "# Code to scrape job information from the website goes here\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_page(page_sauce: bytes | str) -> list:\n",
    "    recruitment_list = list ()\n",
    "    soup = BeautifulSoup(page_sauce, 'html.parser')\n",
    "    job_info_set = soup.select(\"body > div.m-content-container > div:nth-child(2) > ul > li\")\n",
    "\n",
    "    # Extract data\n",
    "    for element in job_info_set:\n",
    "        recruitment = dict()\n",
    "\n",
    "        # Locate key node\n",
    "        date_section_tag = element.find(name='section', attrs={'class': 'm-list-line_date'})\n",
    "        content_section_tag = element.find(name='section', attrs={'class': 'm-list-line_content'})\n",
    "        item_section_tag = element.find(name='section', attrs={'class': 'm-list-xyzp_item_top'})\n",
    "        item_ul_tag = element.find(name='ul', attrs={'class': 'm-list-xyzp_item_bottom'})\n",
    "\n",
    "        # Pre-build data structure\n",
    "        date = list(date_section_tag.stripped_strings)\n",
    "        date.reverse()\n",
    "        date = ''.join(date)\n",
    "        \n",
    "        name = content_section_tag.h3.a.string.lstrip()\n",
    "\n",
    "        time_hits_location = ['N/A', 'N/A', 'N/A',]\n",
    "        time_hits_location[0] = list(item_section_tag.stripped_strings)[0].split(' ')[-1]  # 时间\n",
    "        time_hits_location[1] = list(item_section_tag.stripped_strings)[1:][0].split('：')[1] # 点击量\n",
    "        time_hits_location[2] = list(item_section_tag.stripped_strings)[1:][1].split('：')[1] # 地点\n",
    "        \n",
    "        orgname_orgnature_orgindustry = ['N/A', 'N/A', 'N/A',]  # 名称， 性质， 行业\n",
    "        org_item_list = [org_item for org_item in item_ul_tag.children if org_item != '\\n']\n",
    "        for i, li in enumerate(org_item_list):\n",
    "            orgname_orgnature_orgindustry[i] = li.h6.string\n",
    "\n",
    "        # Build a dictionary to store info \n",
    "        try:\n",
    "            recruitment['日期'] = date\n",
    "        except:\n",
    "            recruitment['日期'] = 'N/A'\n",
    "        try:\n",
    "            recruitment['招聘会名称'] = name\n",
    "        except:\n",
    "            recruitment['招聘会名称'] = 'N/A'\n",
    "        recruitment['招聘时间'] = time_hits_location[0]\n",
    "        recruitment['点击量'] = time_hits_location[1]\n",
    "        recruitment['招聘地点'] = time_hits_location[2]\n",
    "        recruitment['单位名称'] = orgname_orgnature_orgindustry[0]\n",
    "        recruitment['单位性质'] = orgname_orgnature_orgindustry[1]\n",
    "        recruitment['单位行业'] = orgname_orgnature_orgindustry[2]\n",
    "        recruitment_list.append(recruitment)\n",
    "        time.sleep(1)\n",
    "    return recruitment_list\n",
    "\n",
    "def main(path, json_file_name, csv_file_name):\n",
    "    job_list = []\n",
    "    # Prepera request\n",
    "    page_no = 1\n",
    "    while True:\n",
    "        url = f\"\"\n",
    "        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.62'\n",
    "        # accept = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "        # accept_encoding = 'gzip, deflate, br'\n",
    "        # accept_language = 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,ja;q=0.5'\n",
    "        # referer = ''\n",
    "        # cookie = 'Hm_lvt_f3a1f3ff8d120912a12ab2de0a25b16a=1693614414; pageSize=12; jdjy.session.id=1eb587a0acb045cd95956b6d37a86c1f; pageNo=63; Hm_lpvt_f3a1f3ff8d120912a12ab2de0a25b16a=1693642813'\n",
    "\n",
    "        headers = {\"User-Agent\": user_agent,\n",
    "                # \"Accept\": accept,\n",
    "                # \"Accept-Encoding\": accept_encoding,\n",
    "                # \"Accept-Language\": accept_language,\n",
    "                # \"Referer\": referer,\n",
    "                # \"Cookie\": cookie\n",
    "                }\n",
    "        \n",
    "\n",
    "        response = requests.get(url, headers=headers, )\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        job_list += parse_page(response.text)\n",
    "        \n",
    "        # Go to next page \n",
    "        # print(soup)\n",
    "        state = soup.find(attrs={'class': 'page-link'}, string='下一页 »').has_attr('onclick')\n",
    "\n",
    "        if not state:\n",
    "            break\n",
    "        else:\n",
    "            page_no += 1\n",
    "            # print(page_no)\n",
    "        \n",
    "\n",
    "    json_file = ''\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(job_list, f, ensure_ascii=False)\n",
    "\n",
    "    job_df = pd.DataFrame.from_records(job_list)\n",
    "    job_df.to_csv(path, index=False)\n",
    "    pprint(job_list)\n",
    "    # # Save page source\n",
    "    # with open('zhaopin.html', 'wb') as fr:\n",
    "    #     fr.write(response.content)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iNLP",
   "language": "python",
   "name": "inlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
